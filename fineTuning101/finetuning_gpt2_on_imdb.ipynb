{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80998153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f46a40d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. LOAD THE BASE MODEL\n",
    "# We pick a \"pre-trained\" model that already knows English basics.\n",
    "model_id = \"gpt2\" # You can replace this with bigger models like Llama-3\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9370df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. PREPARE THE DATASET\n",
    "# This is a sample dataset. For real fine-tuning, you would load your own\n",
    "# custom .json or .csv files here.\n",
    "dataset = load_dataset(\"imdb\", split=\"train[:1%]\") # Using 1% for a quick demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aee538fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. CONFIGURE THE TRAINING SETTINGS\n",
    "# These are the \"knobs\" we turn to control how the model learns.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./my_special_model\", # Where to save the model\n",
    "    per_device_train_batch_size=4,   # How many examples to show the AI at once\n",
    "    num_train_epochs=3,              # How many times to loop through the data\n",
    "    learning_rate=2e-5,              # How \"fast\" the model should update its brain\n",
    "    logging_steps=10,                # Print progress every 10 steps\n",
    "    save_strategy=\"epoch\",           # Save the model after every loop\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4acb2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2ef03f4eb54f30aca3cf4f5547995a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. START THE FINE‑TUNING\n",
    "# The Trainer acts like a \"teacher\" that shows the data to the model.\n",
    "\n",
    "# 1. tokenize the text column\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "tokenized = dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "# causal‑LM training: labels = input_ids\n",
    "tokenized = tokenized.map(lambda ex: {\"labels\": ex[\"input_ids\"]}, batched=True)\n",
    "\n",
    "# we don't need the original text/label columns any more\n",
    "tokenized = tokenized.remove_columns([\"text\", \"label\"])\n",
    "\n",
    "# 2. create the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c524528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bisha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 42:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.861300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.889200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.907800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.753200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.696000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.615100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.723500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.670700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.616800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.574800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.644300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.541200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>3.590700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>3.600600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.521100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>3.507500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>3.523700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>3.629400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bisha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\bisha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# This command starts the actual retraining process\n",
    "trainer.train()\n",
    "\n",
    "# 5. SAVE YOUR NEW BRAIN\n",
    "# Now you have a version of GPT-2 that understands your specific data better.\n",
    "trainer.save_model(\"./my_special_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d72db93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie is a satire of the American dream. It is a satire of a fantasy movie that has been made by the very real people who made it. It is a satire of the dream of the American dream. It has been made by the very real people who made it. Now, the Hollywood elite have taken control of the movie industry and they have turned it into a propaganda film. There is nothing to be said about that. It is a propaganda film. It is a propaganda film that has been made by the very real people who made it. The film is a propaganda film. It has been made by the very real people who made it. The film is a propaganda film. It has been made by the very real people who made it. This is an interesting movie. It is a fascinating movie. It is a fascinating movie that has a lot of plot holes. There are very few plot holes. The movie is a comedy. It is a comedy with a lot of plot holes. The movie is a comedy with a lot of plot holes. It is a comedy. It is a comedy with a lot of plot holes. There are very few plot holes. It is a comedy with a lot of plot holes. This is an interesting movie. It is a comedy with a lot\n",
      "This movie is a bit of a mess. The plot is a bit of a mess, and the characters are all over the place. The plot is a bit of a mess, and the characters are all over the place. The plot is a bit\n"
     ]
    }
   ],
   "source": [
    "# 6. USE YOUR FINE-TUNED MODEL\n",
    "# Load the fine-tuned model and tokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load your fine-tuned model\n",
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\"./my_special_model\")\n",
    "fine_tuned_tokenizer = AutoTokenizer.from_pretrained(\"./my_special_model\")\n",
    "\n",
    "# Method 1: Using the pipeline (easier)\n",
    "generator = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=fine_tuned_tokenizer)\n",
    "\n",
    "# Generate text\n",
    "prompt = \"This movie is\"\n",
    "output = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "print(output[0]['generated_text'])\n",
    "\n",
    "# Method 2: Manual generation (more control)\n",
    "input_ids = fine_tuned_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "output_ids = fine_tuned_model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "generated_text = fine_tuned_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
